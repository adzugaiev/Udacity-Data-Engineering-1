# Udacity - Data Engineering - 1 - Data Modeling with Postgres

## About / Synopsis

In this project, I apply what I've learned on data modeling with Postgres and build an ETL pipeline using Python. To complete the project, I need to define fact and dimension tables for a star schema for a particular analytic focus, and write an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL.

## Table of Contents
* [Project Datasets](#project-datasets)
    - [Song Dataset](#song-dataset)
    - [Log Dataset](#log-dataset)
* [Schema for Song Play Analysis](schema-for-song-play-analysis)
    - [Fact Table](#fact-table)
    - [Dimension Tables](#dimension-tables)
* [Files in the Project](#files-in-the-project)
* [Running the Project](#running-the-project)
* [What I Have Learned](#what-i-have-learned)
* [Author](#author)

## Project Datasets

### Song Dataset

The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, `song_data/A/B/C/TRABCEI128F424C983.json`

### Log Dataset

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations. The log files in the dataset are partitioned by year and month. For example, `log_data/2018/11/2018-11-12-events.json`.

## Schema for Song Play Analysis

Using the song and log datasets, I will to create a star schema optimized for queries on song play analysis. This includes the following tables.

### Fact Table

* **songplays** - records in log data associated with song plays i.e. records with page `NextSong`
    - *songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*

### Dimension Tables

* **users** - users in the app
    - *user_id, first_name, last_name, gender, level*
* **songs** - songs in music database
    - *song_id, title, artist_id, year, duration*
* **artists** - artists in music database
    - *artist_id, name, location, latitude, longitude*
* **time** - timestamps of records in songplays broken down into specific units
    - *start_time, hour, day, week, month, year, weekday*

## Files in the Project

- `test.ipynb` displays the first few rows of each table to let you check your database.
- `create_tables.py` drops and creates tables. Run this file to reset your tables before each time you run your ETL scripts.
- `etl.ipynb` reads and processes a single file from `song_data` and `log_data` and loads the data into your tables. This notebook contains detailed instructions on the ETL process for each of the tables.
- `etl.py` reads and processes files from `song_data` and `log_data` and loads them into your tables. You can fill this out based on your work in the ETL notebook.
- `sql_queries.py` contains all your sql queries, and is imported into the last three files above.
- `README.md` provides discussion on your project.

## Running the Project

> You will not be able to run `test.ipynb`, `etl.ipynb`, or `etl.py` until you have run `create_tables.py` at least once to create the `sparkifydb` database, which these other files connect to.

## What I Have Learned

Through the implementation of this project, while solving the project's core tasks, I've learned some new things:

1) Using upsert with fields like `serial` and `timestamp`;
1) Tried to implement time dimensions as generated columns, only to find out the version of PostgreSQL in the project space is not supporting them;
1) Improved generation of the data files list to exclude checkpoints, see `get_files()`;
1) A number of `pd.DataFrame` manipulations which I don't memorize anyway and google each time from Stackoverflow;
1) *Level* is not a permanent attribute of a user, I think it's incorrect to set this dimension based on the `log_data`. I sorted it descending by timestamp, so at least we have the latest mentioned level of a user;
1) I had to use string for the song length since `WHERE` condition on floating point value proved not reliable to me;
1) Replaced the iterative insert of `log_data` with copy of the `DataFrame` [from the memory](https://naysan.ca/2020/05/09/pandas-to-postgresql-using-psycopg2-bulk-insert-performance-benchmark/) using `StringIO`. Copy insert is further used in `etl.py`, see `copy_from_stringio()`. I did not measure the difference in performance though. Making the CSV export & import match has drained the remainder of my motivation.

## Author

Andrii Dzugaiev, [in:dzugaev](https://www.linkedin.com/in/dzugaev/)